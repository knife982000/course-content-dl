{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W3D4_ContinualLearning/student/W3D4_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D4_ContinualLearning/student/W3D4_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 2: Out-of-distribution (OOD) Learning\n",
    "\n",
    "**Week 3, Day 4: Continual Learning**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__  Avishree Khare, Het Shah, Joshua Vogelstein\n",
    "\n",
    "__Content reviewers:__ Arush Tagade, Jeremy Forest, Kelson Shilling-Scrivo\n",
    "\n",
    "__Content editors:__ Anoop Kulkarni, Spiros Chavlis\n",
    "\n",
    "__Production editors:__ Deepak Raya, Spiros Chavlis\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
    "\n",
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Tutorial Objectives\n",
    "\n",
    "Deep Learning has seen tremendous growth in recent years thanks to more data, more compute and *very* deep neural networks. Although these networks perform extremely well on the specified task, they fail to generalize to newer tasks. \n",
    "\n",
    "In this tutorial, we will explore Out-of-distribution (OOD) Learning and the several OOD paradigms that have been gaining popularity in recent years. We will understand what OOD really means and how it is different from anything else that we've looked at so far. We'll also take a look at Transfer Learning, Multi-task Learning and Meta-Learning which aim to facilitate OOD Learning in different ways. Here is a list of topics that we would be covering in this tutorial:\n",
    "\n",
    "1. Introduction to OOD Learning\n",
    "2. Transfer Learning\n",
    "3. Multi-Task Learning\n",
    "4. Meta-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "\n",
    "# @markdown These are the slides for the videos in this tutorials\n",
    "\n",
    "# @markdown If you want to locally download the slides, click [here](https://osf.io/dqnm6/download)\n",
    "from IPython.display import IFrame\n",
    "IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/dqnm6/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "!pip install Pillow --quiet\n",
    "!pip install pandas --quiet\n",
    "\n",
    "!pip install git+https://github.com/NeuromatchAcademy/evaltools --quiet\n",
    "from evaltools.airtable import AirtableForm\n",
    "\n",
    "# generate airtable form\n",
    "atform = AirtableForm('appn7VdPRseSoMXEG','W3D4_T2','https://portal.neuromatchacademy.org/api/redirect/to/1d7fcd5d-f1e9-4ac5-ae58-b0ade54a4f87')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchvision.datasets import CelebA, Omniglot\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "import ipywidgets as widgets       # interactive display\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "\n",
    "def visualize_siamese_sample(sample):\n",
    "  to_PIL = transforms.ToPILImage()\n",
    "  fig, axs = plt.subplots(1, 2, figsize=(8, 10))\n",
    "  x1, x2, y = sample\n",
    "  label = y.item()\n",
    "  similarity = \"Same character\" if label == 1.0 else \"Different characters\"\n",
    "  print(f\"-------------------- Label: {label} ({similarity}) -------------------\")\n",
    "  axs[0].imshow(to_PIL(x1), cmap='gray')\n",
    "  axs[1].imshow(to_PIL(x2), cmap='gray')\n",
    "  # axs[0].set_title(f'Label: {label} ({similarity})')\n",
    "\n",
    "  for i in range(2):\n",
    "    axs[i].xaxis.set_ticks([])\n",
    "    axs[i].yaxis.set_ticks([])\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def visualize_mtfl_sample(sample):\n",
    "  to_PIL = transforms.ToPILImage()\n",
    "  img, labels = sample\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.xaxis.set_ticks([])\n",
    "  ax.yaxis.set_ticks([])\n",
    "  print(f\"Labels: {labels.tolist()}\")\n",
    "  ax.imshow(to_PIL(img))\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def visualize_one_shot_sample(sample):\n",
    "  to_PIL = transforms.ToPILImage()\n",
    "  query_img, support_imgs, support_labels, similarity = sample\n",
    "  n_rows = 1\n",
    "  n_cols = len(support_imgs) + 1\n",
    "  fig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols*4, 10))\n",
    "\n",
    "  axs[0].imshow(to_PIL(query_img), cmap='gray')\n",
    "  axs[0].set_title('Query Image')\n",
    "  axs[0].xaxis.set_ticks([])\n",
    "  axs[0].yaxis.set_ticks([])\n",
    "\n",
    "  for i, (s_img, s_label) in enumerate(zip(support_imgs, support_labels)):\n",
    "    axs[i+1].imshow(to_PIL(s_img), cmap='gray')\n",
    "    label = s_label.item()\n",
    "    similarity = \"Same character\" if label == 1.0 else \"Different characters\"\n",
    "    axs[i+1].set_title(f\"Support Image {i+1} \\n \"\n",
    "                       f\"Label: {s_label.item()} ({similarity})\")\n",
    "    axs[i+1].xaxis.set_ticks([])\n",
    "    axs[i+1].yaxis.set_ticks([])\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Dataset Definitions and Helper Functions\n",
    "\n",
    "# @markdown `pandas` and `PIL` libraries should be installed.\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "\"\"\"\n",
    "Datasets for N-way 1-shot classification on the Omniglot Dataset\n",
    "\"\"\"\n",
    "\n",
    "class SiameseOmniglotDataset(Dataset):\n",
    "  def __init__(self, num_samples=10000,\n",
    "               data_transforms=transforms.ToTensor(), download=True):\n",
    "    self.dataset = Omniglot(root='./data/',\n",
    "                            background=True,\n",
    "                            download=download,\n",
    "                            transform=data_transforms)\n",
    "    self.num_samples = num_samples\n",
    "    self.num_classes = len(self.dataset._characters)\n",
    "    self.classes = range(self.num_classes)\n",
    "    self.instances_per_class = dict()\n",
    "    self.get_instances_per_class()\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    if idx % 2 == 0:\n",
    "      return self.get_instances_from_same_class()\n",
    "    return self.get_instances_from_diff_classes()\n",
    "\n",
    "  def get_instances_from_same_class(self):\n",
    "    c = random.randint(0, self.num_classes-1)\n",
    "    [ch_1, ch_2] = random.sample(self.instances_per_class[c], 2)\n",
    "    return ch_1, ch_2, torch.tensor([1.0])\n",
    "\n",
    "  def get_instances_from_diff_classes(self):\n",
    "    [c1, c2] = random.sample(self.classes, 2)\n",
    "    [ch_1] = random.sample(self.instances_per_class[c1], 1)\n",
    "    [ch_2] = random.sample(self.instances_per_class[c2], 1)\n",
    "    return ch_1, ch_2, torch.tensor([0.0])\n",
    "\n",
    "  def get_random_instance(self):\n",
    "    ids = np.random.randint(0, len(self.dataset), 2)\n",
    "    character_1 = self.dataset[ids[0]][0]\n",
    "    character_2 = self.dataset[ids[1]][0]\n",
    "    label_1 = self.dataset[ids[0]][1]\n",
    "    label_2 = self.dataset[ids[1]][1]\n",
    "    similarity = torch.tensor([label_1 == label_2], dtype=torch.float)\n",
    "    return character_1, character_2, similarity\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.num_samples\n",
    "\n",
    "  def get_instances_per_class(self):\n",
    "    for (image, label) in self.dataset:\n",
    "      if label in self.instances_per_class:\n",
    "        self.instances_per_class[label].append(image)\n",
    "      else:\n",
    "        self.instances_per_class[label] = [image]\n",
    "\n",
    "\n",
    "class NWayOneShotOmniglotDataset(Dataset):\n",
    "  def __init__(self, data_transforms=transforms.ToTensor(), n_ways=5,\n",
    "               download=True):\n",
    "    self.dataset = Omniglot(root='./data/',\n",
    "                            background=False,\n",
    "                            download=download,\n",
    "                            transform=data_transforms)\n",
    "    self.size = len(self.dataset)\n",
    "    self.num_classes = len(self.dataset._characters)\n",
    "    self.classes = range(self.num_classes)\n",
    "    self.n_ways = n_ways\n",
    "    self.instances_per_class = dict()\n",
    "    self.get_instances_per_class()\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    query_img, query_class = self.dataset[idx]\n",
    "\n",
    "    # Find (n_ways - 1) distinct characters different from query_class\n",
    "    support_imgs = []\n",
    "    support_labels = []\n",
    "    support_classes = random.sample([c for c in self.classes if c != query_class], self.n_ways-1)\n",
    "\n",
    "    # Find 1 support image from query_class\n",
    "    support_classes.append(query_class)\n",
    "\n",
    "    random.shuffle(support_classes)\n",
    "\n",
    "    for c in support_classes:\n",
    "      [img] = random.sample(self.instances_per_class[c], 1)\n",
    "      support_imgs.append(img)\n",
    "      support_labels.append(torch.tensor([c == query_class], dtype=torch.float))\n",
    "\n",
    "    _, similarity = torch.max(torch.tensor(support_labels), axis=0)\n",
    "    return query_img, support_imgs, support_labels, similarity\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.size\n",
    "\n",
    "  def get_instances_per_class(self):\n",
    "    for (image, label) in self.dataset:\n",
    "      if label in self.instances_per_class:\n",
    "        self.instances_per_class[label].append(image)\n",
    "      else:\n",
    "        self.instances_per_class[label] = [image]\n",
    "\n",
    "\n",
    "class MTFLDataset(Dataset):\n",
    "  def __init__(self, data_file,\n",
    "               num_samples=10000,\n",
    "               data_transforms=transforms.ToTensor(), ):\n",
    "\n",
    "    self.df = pd.read_csv(data_file, sep=' ', header=None,\n",
    "                          skipinitialspace=True, nrows=num_samples)\n",
    "    self.df.iloc[:, 0] = self.df.iloc[:, 0].apply(lambda s: s.replace('\\\\', '/'))\n",
    "    self.transform = data_transforms\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = self.df.iloc[idx]\n",
    "    img_name = item[0]\n",
    "    labels = (item[11:] - 1)  # 1-indexed to 0-indexed\n",
    "\n",
    "    img = Image.open(img_name)\n",
    "    img = self.transform(img)\n",
    "\n",
    "    return img, torch.from_numpy(np.array(labels, dtype=np.float32)).long()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load datasets\n",
    "\"\"\"\n",
    "\n",
    "def get_train_val_datasets(background_dataset_size=10000,\n",
    "                           val_split=0.2, download=True):\n",
    "\n",
    "  dataset_size = background_dataset_size\n",
    "  val_split = 0.2\n",
    "  train_size = int(dataset_size * (1 - val_split))\n",
    "  val_size = dataset_size - train_size\n",
    "\n",
    "  background_dataset = SiameseOmniglotDataset(num_samples=dataset_size,\n",
    "                                              download=download)\n",
    "  train_dataset, val_dataset = random_split(background_dataset, [train_size, val_size])\n",
    "\n",
    "  return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def get_test_dataset(n_ways=5, download=True):\n",
    "  return NWayOneShotOmniglotDataset(n_ways=n_ways, download=download)\n",
    "\n",
    "\n",
    "def get_train_val_datasets_mtfl(dataset_size=10000, val_split=0.2):\n",
    "\n",
    "  data_transforms = transforms.Compose([transforms.Resize((256, 256)),\n",
    "                                        transforms.ToTensor()])\n",
    "\n",
    "  train_size = int(dataset_size * (1 - val_split))\n",
    "  val_size = dataset_size - train_size\n",
    "\n",
    "  background_dataset = MTFLDataset(data_file='/content/MTFL/training.txt',\n",
    "                                   num_samples=dataset_size,\n",
    "                                   data_transforms=data_transforms)\n",
    "\n",
    "  train_dataset, val_dataset = random_split(background_dataset, [train_size, val_size])\n",
    "\n",
    "  return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Training/Evaluation Helper Functions\n",
    "\n",
    "# @markdown `train_multi_task(model, trainloader, valloader, criterion, optimizer, epochs=10, device='cpu')`\n",
    "def train_multi_task(model, trainloader, valloader, criterion, optimizer,\n",
    "                     epochs=10, device='cpu'):\n",
    "\n",
    "  best_model_weights = copy.deepcopy(model.state_dict())\n",
    "  best_overall_acc = 0.0\n",
    "\n",
    "  for ep in range(epochs):\n",
    "    print(\"\")\n",
    "    print(\"======== Epoch {:} / {:} ========\".format(ep + 1, epochs))\n",
    "\n",
    "    running_corrects = np.zeros(4)\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    for inps, labels in trainloader:\n",
    "      inps = inps.to(device)\n",
    "      labels = labels.to(device)\n",
    "      outs = model(inps)\n",
    "      overall_loss = 0.0\n",
    "\n",
    "      for i in range(4):\n",
    "        overall_loss += criterion(outs[i].to(device), labels[:, i].to(device))\n",
    "        _, preds = torch.max(outs[i], 1)\n",
    "        running_corrects[i] += torch.sum(preds == labels[:, i].data)\n",
    "\n",
    "      running_loss += overall_loss.item()\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      overall_loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    train_loss = running_loss/(len(trainloader) * inps.size(0))\n",
    "    train_accs = running_corrects / (len(trainloader) * inps.size(0))\n",
    "    train_overall_acc = np.mean(train_accs)\n",
    "    running_corrects_val = np.zeros(4)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for inps, labels in valloader:\n",
    "        inps = inps.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outs = model(inps)\n",
    "\n",
    "        overall_loss = 0.0\n",
    "\n",
    "        for i in range(4):\n",
    "          _, preds = torch.max(outs[i], 1)\n",
    "          running_corrects_val[i] += torch.sum(preds == labels[:, i].data)\n",
    "\n",
    "      val_accs = running_corrects_val / (len(valloader) * inps.size(0))\n",
    "\n",
    "      val_overall_acc = np.mean(val_accs)\n",
    "\n",
    "      if val_overall_acc > best_overall_acc:\n",
    "        best_overall_acc = val_overall_acc\n",
    "        best_model_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print(f\"Training => Avg. Task Accuracy: {train_overall_acc}\")\n",
    "    print(f\"Validation => Avg. Task Accuracy: {val_overall_acc}\")\n",
    "\n",
    "  model.load_state_dict(best_model_weights)\n",
    "  return model\n",
    "\n",
    "\n",
    "# @markdown `train_siamese_network(model, criterion, optimizer, train_loader, device='cpu', print_freq=100)`\n",
    "def train_siamese_network(model, criterion, optimizer, train_loader,\n",
    "                          device=\"cpu\", print_freq=100):\n",
    "  model.to(device)\n",
    "  running_loss = 0.0\n",
    "  correct = 0.0\n",
    "  total = 0.0\n",
    "  for batch_idx, data in enumerate(train_loader):\n",
    "    x1 = data[0].to(device)\n",
    "    x2 = data[1].to(device)\n",
    "    y = data[2].to(device)\n",
    "\n",
    "    optimizer.zero_grad() #Set parameter gradients to zero\n",
    "\n",
    "    y_pred = model(x1, x2)\n",
    "    # print(y_pred.shape, y.shape)\n",
    "    loss = criterion(y_pred.view(-1), y.view(-1))\n",
    "    loss.backward() #Set the values for gradients (`grad`) of all the parameters\n",
    "    optimizer.step() #Update the parameter values using the computed gradients\n",
    "\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    y_out = torch.round(torch.sigmoid(y_pred)) #To do: do necessary modifs from sigmoid to regular\n",
    "    correct += y_out.eq(y.to(device)).sum()\n",
    "    total += y_out.shape[0]\n",
    "\n",
    "    # if (batch_idx % print_freq) == (print_freq - 1):\n",
    "    #   print(f\"Batch: {batch_idx} Running loss: {running_loss / print_freq} Train accuracy: {acc}\")\n",
    "    #   running_loss = 0.0\n",
    "\n",
    "  print(f\"Training => Average Loss: {running_loss/len(train_loader)} | Accuracy: {correct/total}\")\n",
    "\n",
    "\n",
    "# @markdown `evaluate_siamese_network(model, criterion, val_loader, device='cpu')`\n",
    "def evaluate_siamese_network(model, criterion, val_loader, device='cpu'):\n",
    "  model.eval()\n",
    "  model.to(device)\n",
    "  correct = 0.0\n",
    "  total = 0.0\n",
    "  val_loss = 0.0\n",
    "  with torch.no_grad():\n",
    "    for (x1, x2, y) in val_loader:\n",
    "      y_pred = model(x1.to(device), x2.to(device))\n",
    "      loss = criterion(y_pred.view(-1), y.to(device).view(-1))\n",
    "      val_loss += loss.item()\n",
    "      y_out = torch.round(torch.sigmoid(y_pred)) #To do: do necessary modifs from sigmoid to regular\n",
    "      correct += y_out.eq(y.to(device)).sum()\n",
    "      total += y_out.shape[0]\n",
    "\n",
    "  val_acc = correct/total\n",
    "  val_loss = val_loss/len(val_loader)\n",
    "\n",
    "  print(f\"Validation => Average Loss: {val_loss} | Accuracy: {val_acc}\")\n",
    "\n",
    "def evaluate_one_shot(model, test_loader, device='cpu'):\n",
    "  model.eval()\n",
    "  model.to(device)\n",
    "\n",
    "  correct = 0.0\n",
    "  total = 0.0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for (query_img, support_imgs, support_labels, similarity) in test_loader:\n",
    "      x_query = query_img.to(device)\n",
    "      y_pred_fc = [model(x_query, x_support.to(device)) for x_support in support_imgs]\n",
    "      y_pred = [torch.sigmoid(pred) for pred in y_pred_fc]\n",
    "      _, y_out = torch.max(torch.cat(y_pred, dim=1), 1)\n",
    "      correct += y_out.eq(similarity.to(device)).sum()\n",
    "      total += y_out.shape[0]\n",
    "\n",
    "  print(f'Testing (One-shot) => Accuracy: {correct/total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# for DL its critical to set the random seed so that students can have a\n",
    "# baseline to compare their results to expected results.\n",
    "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU). Execute `set_device()`\n",
    "# especially if torch modules used.\n",
    "\n",
    "# inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 1: Introduction to Out-of-Distribution (OOD) Learning\n",
    "\n",
    "*Time estimate: ~5mins*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this section, we'll take a brief look at what OOD Learning is and how it can be useful in solving various problems with traditional Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 1: The Future of Learning\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1764y167fj\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"zMQySpHBoGs\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# add event to airtable\n",
    "atform.add_event('Video 1: The Future of Learning')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Think! 1.1: What are the problems that *you* want to solve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Think of a problem that you would want to have solved using Deep Learning. Don't restrict yourself to the problems you have looked at so far! \n",
    "\n",
    "Take 2 mins to think in silence and jot down a brief description of the problem here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown\n",
    "Problem_that_you_want_to_solve = '' # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now discuss the following as a group:\n",
    "\n",
    "1. Can these problems be solved using the techniques that you have looked at so far? If not, why?\n",
    "2. Is there an OOD element attached to any of the problems, which makes them harder to solve? \n",
    "3. Can you think of ways you could solve these problems? (Don't worry if you struggle, we'll hopefully find some suggestions by the end of this tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 2: Transfer and Multi-Task Learning\n",
    "\n",
    "*Time estimate: ~35mins*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this tutorial we will learn transfer learning and multi-task learning. Transfer learning is a machine learning method where a model which is trained on a task (or dataset) can be used as a good initialization for training on a completely different and unrelated task (or dataset). Transfer learning is a very commonly used practice in a lot of Deep Learning works. This method is not limited to any particular subdomain and is widely used in almost all the domains, namely Computer Vision, Natural Language Processing, Reinforcement Learning, etc. \n",
    "\n",
    "Multi-task learning aims to learn multiple tasks simultaneously using shared knowledge across these tasks. We can use Transfer learning to initialize the shared parts of the network. \n",
    "\n",
    "We aim to learn these concepts via a simple problem of attributes classification for the celebA dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 2: Transfer and Multi-Tasking Learning\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV18w41197o8\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"PplDj0vXuIY\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# add event to airtable\n",
    "atform.add_event('Video 2: Transfer and Multi-Tasking Learning')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.1: Getting the data\n",
    "We will be using the [MTFL](http://mmlab.ie.cuhk.edu.hk/projects/TCDCN.html) dataset for demonstrating the concepts of Transfer and Multi-Task Learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Download and unzip the dataset\n",
    "import requests, zipfile\n",
    "\n",
    "# originally from 'http://mmlab.ie.cuhk.edu.hk/projects/TCDCN/data/MTFL.zip'\n",
    "os.chdir('/content')\n",
    "name = 'MTFL'\n",
    "fname = f\"{name}.zip\"\n",
    "url = \"https://osf.io/u5emj/download\"\n",
    "\n",
    "if not os.path.exists(name):\n",
    "  print(\"Start downloading and unzipping `MTFL` dataset...\")\n",
    "  r = requests.get(url, allow_redirects=True)\n",
    "  with open(fname, 'wb') as fd:\n",
    "    fd.write(r.content)\n",
    "  with zipfile.ZipFile(fname, 'r') as zip_ref:\n",
    "    zip_ref.extractall(name)\n",
    "  print(\"Download completed.\")\n",
    "else:\n",
    "  print('Data has been already downloaded.')\n",
    "\n",
    "print('Change direcrtory!')\n",
    "os.chdir(name)\n",
    "print(f'Current dir: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Lets load the dataset into dataloaders which will help us in training the model! \n",
    "\n",
    "You can check the implementation  of the `get_train_val_datasets_mtfl()` in the hidden cell `Dataset Definition and Helper Functions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create dataloaders for the train and validation datasets\n",
    "\"\"\"\n",
    "\n",
    "train_dataset, val_dataset = get_train_val_datasets_mtfl()\n",
    "\n",
    "# Change this for a different batch size\n",
    "batch_size = 16\n",
    "\n",
    "g_seed = torch.Generator()\n",
    "g_seed.manual_seed(SEED)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=2,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g_seed\n",
    "    )\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=2,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g_seed\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We visualize the samples from the dataset, we can see  that the input will be an image and there are multiple labels for the same image. The description of each label - \n",
    "\n",
    " - gender : 0 - male, 1 - female\n",
    " - smiling : 0 - yes, 1 - no\n",
    " - wearing glasses : 0 - yes, 1 - no\n",
    " - head pose : 0° - 0, +30° - 1, -30° - 2, +60° - 3, -60° - 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "sample = train_dataset[0]\n",
    "visualize_mtfl_sample(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Can you interpret the above image and the labels corresponding to the image? Let's have a look at some more instances from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D4_ContinualLearning/static/labels_cl.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.2: Defining the model \n",
    "\n",
    "A typical multi-task model looks like below, with shared layers being layers where knowledge is shared. Task-specific layers, as the name suggests, are specific to the tasks. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D4_ContinualLearning/static/model_cl.png\">\n",
    "\n",
    "\n",
    "For the shared layers we will use a pre-trained backbone network. Here you can try out various models, we have provided an example of using resnet18 backbone. We have to remove the last fully connected layer, because that is specifically used for classificiation task. Futher, for each task we have some fully connected layers, you can play around with more layers on your own! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2.2: Creating a Multi-Task model\n",
    "\n",
    "Complete the custom model class `Multi_task_model`. \n",
    "Complete the `forward()` function, by adding your solution to collect outputs for all the tasks using the fully connected for the corresponding task. \n",
    "\n",
    "You can find an entire list of pre-trained models provided by PyTorch [here](https://pytorch.org/vision/stable/models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class Multi_task_model(nn.Module):\n",
    "  def __init__(self, pretrained=True, num_tasks=4, load_file=None,\n",
    "               num_labels_per_task=[2, 2, 2, 5]):\n",
    "    super(Multi_task_model, self).__init__()\n",
    "    self.backbone = models.resnet18(pretrained=pretrained)  # You can play around with different pre-trained models\n",
    "    if load_file:\n",
    "      self.backbone.load_state_dict(torch.load(load_file))\n",
    "    self.backbone = torch.nn.Sequential(*(list(self.backbone.children())[:-1]))  # Remove the last fully connected layer\n",
    "\n",
    "    if pretrained:\n",
    "      for param in self.backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    self.fcs = []\n",
    "\n",
    "    self.num_tasks = num_tasks\n",
    "\n",
    "    for i in range(self.num_tasks):\n",
    "      self.fcs.append(nn.Sequential(\n",
    "          nn.Linear(512, 128),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.4),\n",
    "          nn.Linear(128, num_labels_per_task[i]),\n",
    "          ################################\n",
    "          # Add more layers if you want! #\n",
    "          ################################\n",
    "          nn.Softmax(dim=1),\n",
    "      ))\n",
    "\n",
    "      self.fcs = nn.ModuleList(self.fcs)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.backbone(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    outs = []\n",
    "    #################################################\n",
    "    # Collect outputs for each task\n",
    "    # Fill in missing code below (...),\n",
    "    # then remove or comment the line below to test your implementation\n",
    "    raise NotImplementedError(\"Complete the model!\")\n",
    "    #################################################\n",
    "    for i in range(...):\n",
    "      outs.append(...)\n",
    "    return outs\n",
    "\n",
    "\n",
    "# add event to airtable\n",
    "atform.add_event('Coding Exercise 2.2: Creating a Multi-Task model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D4_ContinualLearning/solutions/W3D4_Tutorial2_Solution_0a937c07.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "To avoid any major blackout due to multiple downloads, we download a pretrained ResNet model locally.\n",
    "\n",
    "**Note:** If `pretrained=False` and `load_file` exists, we load the pretrained model from a file. If you want to use a different model, set `load_file` to `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Download `resnet18` pretrained\n",
    "url = \"https://osf.io/2kd98/download\"\n",
    "fname = \"resnet18-f37072fd.pth\"\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "\n",
    "with open(fname, 'wb') as fd:\n",
    "  fd.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialize two models with and without pre-trained weights\n",
    "\"\"\"\n",
    "model_with_pre_trained_backbone = Multi_task_model(pretrained=False,\n",
    "                                                   load_file=fname).to(DEVICE)\n",
    "model_without_pre_trained_backbone = Multi_task_model(pretrained=False).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's define the loss function and optimizer. Since this is a classification task, we will be using cross-entropy loss. For the optimizer we will be using Stochastic Gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_without_pre_trained_backbone = optim.SGD(\n",
    "    model_without_pre_trained_backbone.parameters(),\n",
    "    lr=0.001,\n",
    "    momentum=0.9\n",
    "    )\n",
    "\n",
    "optimizer_pre_trained_backbone = optim.SGD(\n",
    "    model_with_pre_trained_backbone.parameters(),\n",
    "    lr=0.001,\n",
    "    momentum=0.9\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.3: Training model without pretrained backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "model_without_pre_trained_backbone = train_multi_task(\n",
    "    model_without_pre_trained_backbone,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer_without_pre_trained_backbone,\n",
    "    epochs=5, device=DEVICE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.4: Training model with pretrained backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "model_pre_trained_backbone = train_multi_task(model_with_pre_trained_backbone,\n",
    "                                              train_loader,\n",
    "                                              val_loader,\n",
    "                                              criterion,\n",
    "                                              optimizer_pre_trained_backbone,\n",
    "                                              epochs=5, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.5: Section Summary\n",
    "\n",
    "We can summarize this section as follows\n",
    "\n",
    "* Transfer learning helps in initializing the model better and thus makes learning of the model faster. It also gives better performance compared to the same model that was trained from scratch. Below is a table which contains validation accuracies after 5 epochs (your accuracies can be slightly different when you run the training loops. It will also depend on the number of epochs and the model you have implemented)-\n",
    "\n",
    "\n",
    "|           Method          | Validation Accuracy |\n",
    "|:-------------------------:|:-------------------:|\n",
    "|   With Transfer Learning  |      **0.779**      | \n",
    "| Without Transfer Learning |        0.728        |\n",
    "\n",
    "* Multi-task learning helps us learn multiple tasks together, by sharing knowledge across layers. Task specific layers then help the model learn task specific features. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 3: Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 3: Interpretability\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1Ey4y1j7Jj\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"P_QtnN1_wMM\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# add event to airtable\n",
    "atform.add_event('Video 3: Interpretability')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Lets' find the most important feature for Cancer prediction and see if it belongs to the set of two most important features for the same problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 3.1: Defining the problem setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Imagine that you want to apply all the knowledge that you have gained from this course to a real health problem, say predicting cancer. You develop an algorithm for predicting cancer using certain input features from patient data. \n",
    "\n",
    "This is a composite model that takes in three binary variables: does the patient have a family history of cancer (1=yes, 0=no), does the patient smoke (1=yes, 0=no), and is the patient young (age < 40) (1=yes, 0=no).\n",
    "\n",
    "The algorithm does not favour either of the outcomes, so $P(cancer) = P(no\\_cancer) = .5$\n",
    "\n",
    "You wish to find the most important feature according to the model and you have the following list of conditional probabilities.\n",
    "\n",
    "\\\\\n",
    "\n",
    "\\begin{align}\n",
    "P(family\\_history = 1  | no\\_cancer) = .1 &, P(family\\_history = 1 | cancer) = .9 \\\\\n",
    "P(smoker = 1 | no\\_cancer) = .05 &, P(smoker = 1| cancer) = .8 \\\\\n",
    "P(young = 1 | no\\_cancer) = .01 &, P(young = 1 | cancer) = .71\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 3.2: Defining a feature importance metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's define a metric that can help us quantify how important a feature (or a given set of features) is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Think! 3.2.1: Think of a suitable feature importance metric!\n",
    "\n",
    "Take a moment to think about feature importance and a simple metric for quantifying it! Remember that the feature importance should depend on how useful the feature is for predicting the outcome *alone*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Student Response\n",
    "from ipywidgets import widgets\n",
    "\n",
    "\n",
    "text=widgets.Textarea(\n",
    "   value='Type your answer here and click on `Submit!`',\n",
    "   placeholder='Type something',\n",
    "   description='',\n",
    "   disabled=False\n",
    ")\n",
    "\n",
    "button = widgets.Button(description=\"Submit!\")\n",
    "\n",
    "display(text,button)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "   atform.add_answer('q1' , text.value)\n",
    "   print(\"Submission successful!\")\n",
    "\n",
    "\n",
    "button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now discuss the following with the group: \n",
    "\n",
    "1. Each member of the group should briefly talk about the feature metric they came up with and why they think it is suitable for the given problem setup.\n",
    "2. Identify the most important characterics of a good feature importance metric. (As different members of the group used different criteria to select their metrics, find out the most important criteria that helped!). Try to limit yourself to the two most important characteristics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Note down the most important characteristic here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Student Response\n",
    "from ipywidgets import widgets\n",
    "\n",
    "\n",
    "text=widgets.Textarea(\n",
    "   value='Type the most important characteristic and click on `Submit!`',\n",
    "   placeholder='Type something',\n",
    "   description='',\n",
    "   disabled=False\n",
    ")\n",
    "\n",
    "button = widgets.Button(description=\"Submit!\")\n",
    "\n",
    "display(text,button)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "   atform.add_answer('q2' , text.value)\n",
    "   print(\"Submission successful!\")\n",
    "\n",
    "\n",
    "button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Also note down the second most important characteristic here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Student Response\n",
    "from ipywidgets import widgets\n",
    "\n",
    "\n",
    "text=widgets.Textarea(\n",
    "   value='Type the second most important characteristic and click on `Submit!`',\n",
    "   placeholder='Type something',\n",
    "   description='',\n",
    "   disabled=False\n",
    ")\n",
    "\n",
    "button = widgets.Button(description=\"Submit!\")\n",
    "\n",
    "display(text,button)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "   atform.add_answer('q3' , text.value)\n",
    "   print(\"Submission successful!\")\n",
    "\n",
    "\n",
    "button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Thank you for coming up with some guidelines for what makes a feature importance metric good!\n",
    "\n",
    "Although the metrics defined by you must be great, we here define a simple metric for maintaining consistency throughout the rest of this tutorial.\n",
    "\n",
    "Let's assume that we wish to find the K best features. We know that a set of K features would be most important if they can predict the outcome with high confidence and better than any other set of the same size! We could hence try to predict the outcome using only the given set of features and record the accuracy. If this accuracy is higher than that using any other set of the same size, then we can say that the given set has the most important K features!\n",
    "\n",
    "In essence, we would want to find the accuracies considering all feature sets of size K and then pick the one with the highest accuracy.\n",
    "\n",
    "If we wish to define a loss instead (the lower the better), we could simply extend the above notion as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "Loss = 1 - Accuracy\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Think! 3.2.2: Is our feature importance metric good?\n",
    "\n",
    "Now that we have provided a new feature importance metric, think of the following: \n",
    "\n",
    "1. What do you think of this new feature importance metric, and how is it different from the one that you came up with? \n",
    "2. Does it satify the criteria that you came up with? \n",
    "3. If not, why? Does the metric need to be modified or the criteria?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Student Response\n",
    "from ipywidgets import widgets\n",
    "\n",
    "\n",
    "text=widgets.Textarea(\n",
    "   value='Type your answer here and click on `Submit!`',\n",
    "   placeholder='Type something',\n",
    "   description='',\n",
    "   disabled=False\n",
    ")\n",
    "\n",
    "button = widgets.Button(description=\"Submit!\")\n",
    "\n",
    "display(text,button)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "   atform.add_answer('q4' , text.value)\n",
    "   print(\"Submission successful!\")\n",
    "\n",
    "\n",
    "button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 3.3: Finding the most important feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Considering the feature importance metric that we defined above, let's try to find the most important and the two most important features!\n",
    "\n",
    "The accuracy can be calculated using the probabilities specified above. Here are the accuracies for different feature sets! (Note that we have not detailed the calculation of these accuracies. If you are curious, refer to the bonus section!)\n",
    "\n",
    "For a detailed explanation, see *Bonus 3*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "|           Feature(s)          | Accuracy |\n",
    "|:---------------------------:|:-------------------:|\n",
    "|   Family History            |        0.90         | \n",
    "| Is a Smoker                 |        0.875        |\n",
    "| Is Young                    |        0.85         |\n",
    "| Family History, Is a Smoker |        0.917        |\n",
    "| Is a Smoker, Is Young       |        0.941        |\n",
    "| Family History, Is Young    |        0.931        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Think! 3.3.1: Finding important features\n",
    "\n",
    "Given the table above, what is the single most important feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Student Response\n",
    "from ipywidgets import widgets\n",
    "\n",
    "\n",
    "text=widgets.Textarea(\n",
    "   value='Type your answer here and click on `Submit!`',\n",
    "   placeholder='Type something',\n",
    "   description='',\n",
    "   disabled=False\n",
    ")\n",
    "\n",
    "button = widgets.Button(description=\"Submit!\")\n",
    "\n",
    "display(text,button)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "   atform.add_answer('q5' , text.value)\n",
    "   print(\"Submission successful!\")\n",
    "\n",
    "\n",
    "button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D4_ContinualLearning/solutions/W3D4_Tutorial2_Solution_240f6cb7.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Can you also find out the second most important features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Student Response\n",
    "from ipywidgets import widgets\n",
    "\n",
    "\n",
    "text=widgets.Textarea(\n",
    "   value='Type your answer here and click on `Submit!`',\n",
    "   placeholder='Type something',\n",
    "   description='',\n",
    "   disabled=False\n",
    ")\n",
    "\n",
    "button = widgets.Button(description=\"Submit!\")\n",
    "\n",
    "display(text,button)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "   atform.add_answer('q6' , text.value)\n",
    "   print(\"Submission successful!\")\n",
    "\n",
    "\n",
    "button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D4_ContinualLearning/solutions/W3D4_Tutorial2_Solution_2566d409.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The most important feature is not one of the two most important features! Can you find out why this is the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 3.4: Section Summary\n",
    "\n",
    "In this section, we discussed the following:\n",
    "\n",
    "* What is Interpretability and why is it important?\n",
    "\n",
    "* How can input features be used to understand the model's predictions (aka feature importances)? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Recall the problem that you set out to solve at the beginning of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown *Run me to print the problem that you wanted to solve!*\n",
    "print(Problem_that_you_want_to_solve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Take a moment to think about the following:\n",
    "\n",
    "1. Did this tutorial help you find some ideas to solve this problem?\n",
    "2. Can you now come up with a solution to this problem by using all the tools that you learnt from this course?\n",
    "\n",
    "We sincerely hope that your answers to these two questions are yes and YES! Recall all that you learnt from this course and how this knowledge and practical experience can help with a problem that *you* want to solve. Go back and experiment with your solutions and see if you can solve this problem that matters to you!    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Airtable Submission Link\n",
    "from IPython import display as IPydisplay\n",
    "IPydisplay.HTML(\n",
    "   f\"\"\"\n",
    " <div>\n",
    "   <a href= \"{atform.url()}\" target=\"_blank\">\n",
    "   <img src=\"https://github.com/NeuromatchAcademy/course-content-dl/blob/main/tutorials/static/SurveyButton.png?raw=1\"\n",
    " alt=\"button link end of day Survey\" style=\"width:410px\"></a>\n",
    "   </div>\"\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Bonus 1: Introduction to Meta Learning\n",
    "\n",
    "*Time estimate: ~20mins*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this section, we will *literally* learn to learn by exploring the concept of Meta Learning. Meta Learning attempts to improve the generalization capabilities of neural networks by teaching them *how to learn new tasks fast*. We aim to introduce you to the following topics:\n",
    "\n",
    "1. Meta-Learning and its applications \n",
    "2. Few-shot classification (the most common application of Meta-Learning in supervised classification settings)\n",
    "3. One-shot Learning with Convolutional Siamese Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 4: Meta Learning\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1zU4y1n77d\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"8grBwtrzcDI\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# add event to airtable\n",
    "atform.add_event('Video 4: Meta Learning')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Meta-Learning is most commonly observed as Few-shot Learning in Supervised learning settings.\n",
    "\n",
    "Few-shot Learning aims to answer the following question: How can a neural network learn a task well with *very little* data? \n",
    "\n",
    "In the context of supervised classification, K-shot learning refers to learning with only K examples of each class. An extension of this would be N-way K-shot learning, which attempts to train a network with only K examples of each of the N classes. Let's take an example of a 5-way 1-shot problem: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D4_ContinualLearning/static/Metalearning.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that we know what Few-shot learning is, let's take a look at a One-shot classification problem using Siamese Networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Bonus Section 1.1: Introduction to Omniglot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The Omniglot data set is a a standardized benchmark for evaluating the performance of Few-shot Learning algorithms. It contains 1623 different handwritten characters from 50 different alphabets. The alphabets range from well-established international languages like Latin and Korean to lesser known local dialects. Fictitious character sets such as Aurek-Besh and Klingon are also included. Here are a few examples from the Omniglot dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D4_ContinualLearning/static/omni.jpeg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Bonus Section 1.2: Convolutional Siamese Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The simplest way to evaluate where an image belongs to a class is by comparing it with other images of the same class. Convolutional Siamese networks help us in comparing two images and quantify how similar or different they are from one another. This is known as verification.\n",
    "\n",
    "Verification of two images is done as follows: Both the images are passed through the convolutional network to generate *feature vectors* representing them. These feature vectors are then compared for similarity. (Find a better image for siamese network maybe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D4_ContinualLearning/static/siamese.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "How can we compare if two vectors are similar? By using a distance metric! **L1 distance** is one such metric that evaluates the distance between two vectors by computing the absolute value of the difference of invidual components of both vectors. More specifically, given vectors `v1` and `v2`, the component-wise L1 distance would be as follows:\n",
    "\n",
    "```\n",
    "l1_distance = abs(v1 - v2)\n",
    "```\n",
    "\n",
    "After this, the problem boils down to a binary classification problem where two images either belong to the same class or not!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Bonus Coding Exercise 1.2: Creating a Convolutional Siamese Network\n",
    "\n",
    "Let's create a Convolutional Siamese Network! The structure of the network is created in the `ConvSiameseNet` class. You have to compute the L1 distance (`l1_distance`) between the feature vectors (`x1_fv` and `x2_fv`). You can use `torch.abs()` to compute the absolute value of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Define the Siamese Network\n",
    "class ConvSiameseNet(nn.Module):\n",
    "  \"\"\"\n",
    "  Convolutional Siamese Network from \"Siamese Neural Networks for One-shot Image Recognition\"\n",
    "  Paper can be found at http://www.cs.toronto.edu/~rsalakhu/papers/oneshot1.pdf\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 64, 10)\n",
    "    self.conv2 = nn.Conv2d(64, 128, 7)\n",
    "    self.conv3 = nn.Conv2d(128, 128, 4)\n",
    "    self.conv4 = nn.Conv2d(128, 256, 4)\n",
    "    self.pool = nn.MaxPool2d(2, 2)\n",
    "    self.fc1 = nn.Linear(256*6*6, 4096)\n",
    "    self.fc2 = nn.Linear(4096, 1)\n",
    "\n",
    "  def model(self, x):\n",
    "    x = self.pool(F.relu(self.conv1(x)))\n",
    "    x = self.pool(F.relu(self.conv2(x)))\n",
    "    x = self.pool(F.relu(self.conv3(x)))\n",
    "    x = F.relu(self.conv4(x))\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = torch.sigmoid(self.fc1(x))\n",
    "    return x\n",
    "\n",
    "  def forward(self, x1, x2):\n",
    "    x1_fv = self.model(x1)\n",
    "    x2_fv = self.model(x2)\n",
    "    ############################################################################\n",
    "    ## TODO: Calculate the component-wise l1_distance between x1_fv and x2_fv ##\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Student exercise: Calculate l1_distance\")\n",
    "    ############################################################################\n",
    "    # Calculate L1 distance (as l1_distance) between x1_fv and x2_fv\n",
    "    l1_distance = ...\n",
    "\n",
    "    return self.fc2(l1_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D4_ContinualLearning/solutions/W3D4_Tutorial2_Solution_696c354e.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Bonus Section 1.3: Verification using Siamese Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's train the siamese network we created on verification of images. We'll take pairs of images from the Omniglot dataset and train the network to identify if the pairs belong to the same class or not.\n",
    "\n",
    "Let's first get the Omniglot dataset with pairs of images. A pair is labelled \"1\" if the images are of the same character, and \"0\" otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Download Omniglot dataset with pairs\n",
    "import zipfile, os, requests\n",
    "\n",
    "# original location: https://github.com/brendenlake/omniglot/tree/master/python\n",
    "os.chdir('/content')\n",
    "print(f'Change dir: {os.getcwd()}')\n",
    "\n",
    "dirname = 'data/omniglot-py/'\n",
    "if not os.path.exists(dirname):\n",
    "  os.makedirs(dirname)\n",
    "\n",
    "fname = 'images_background.zip'\n",
    "url = \"https://osf.io/6hq9u/download\"\n",
    "\n",
    "if not os.path.exists(dirname + 'images_background'):\n",
    "  print('Downlading the dataset...')\n",
    "  r = requests.get(url, allow_redirects=True)\n",
    "  with open(dirname + fname, 'wb') as fd:\n",
    "    fd.write(r.content)\n",
    "  with zipfile.ZipFile(dirname + fname, 'r') as zip_ref:\n",
    "    zip_ref.extractall(dirname)\n",
    "  print('Dataset is downloaded.')\n",
    "else:\n",
    "  print('Dataset has already been downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load train and validation datasets for training the Siamese Network\n",
    "\"\"\"\n",
    "\n",
    "train_dataset, val_dataset = get_train_val_datasets(background_dataset_size=10000, val_split=0.2, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize a sample from the training dataset\n",
    "\"\"\"\n",
    "\n",
    "# Change this to visualize another sample from the dataset\n",
    "sample_idx = 1\n",
    "sample = train_dataset[sample_idx]\n",
    "\n",
    "visualize_siamese_sample(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create dataloaders for the train and validation datasets\n",
    "\"\"\"\n",
    "\n",
    "# Change this for a different batch size\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=2,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g_seed\n",
    "    )\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=2,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g_seed\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that we have the necessary dataloaders, let's train a siamese network! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train the network on Omniglot\n",
    "\"\"\"\n",
    "\n",
    "siamese_net = ConvSiameseNet()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(siamese_net.parameters(), lr=0.0001)\n",
    "\n",
    "# Change this to train for any number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  print(f\"\\n Epoch {epoch + 1} / {num_epochs} ================================\")\n",
    "\n",
    "  train_siamese_network(siamese_net, criterion, optimizer, train_loader, DEVICE)\n",
    "  evaluate_siamese_network(siamese_net, criterion, val_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Bonus Section 1.4: One-shot Classification with Siamese Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that we have a siamese network trained on verification, let's try to extend it to one-shot classification. How can we do this? \n",
    "\n",
    "Let's first try to understand what the current network offers: Given two images, find out if they belong to the same class or not. \n",
    "\n",
    "What does N-way one-shot classification ask for? Given one image from each of the N classes (the support set), find out which class does the query image belong to.\n",
    "\n",
    "Take a moment to think about how the verification network can be extended to work in the one-shot setting.\n",
    "\n",
    "Given a query image, compare it with different images in the support set using our siamese network. Pick the class that is most similar to the query image!\n",
    "\n",
    "Let's get the test dataset where one sample corresponds to a query image and N support images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Download the test dataset\n",
    "import zipfile, os, requests\n",
    "\n",
    "# original location: https://github.com/brendenlake/omniglot/tree/master/python\n",
    "\n",
    "dirname = 'data/omniglot-py/'\n",
    "if not os.path.exists(dirname):\n",
    "  os.makedirs(dirname)\n",
    "\n",
    "fname = 'images_evaluation.zip'\n",
    "url = \"https://osf.io/uq4gw/download\"\n",
    "\n",
    "if not os.path.exists(dirname + 'images_evaluation'):\n",
    "  print('Downlading the dataset...')\n",
    "  r = requests.get(url, allow_redirects=True)\n",
    "  with open(dirname + fname, 'wb') as fd:\n",
    "    fd.write(r.content)\n",
    "  with zipfile.ZipFile(dirname + fname, 'r') as zip_ref:\n",
    "    zip_ref.extractall(dirname)\n",
    "  print('Dataset is downloaded.')\n",
    "else:\n",
    "  print('Dataset has already been downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the test dataset for N-way One-shot classification\n",
    "\"\"\"\n",
    "\n",
    "# Change this to change the number of classes in the support set\n",
    "n_ways = 5\n",
    "\n",
    "test_dataset = get_test_dataset(n_ways=n_ways, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize a sample from the test dataset\n",
    "\"\"\"\n",
    "\n",
    "# Change this to visualize another sample from the dataset\n",
    "sample_idx = 1\n",
    "sample = test_dataset[sample_idx]\n",
    "\n",
    "visualize_one_shot_sample(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We are in the endgame now. Let's look at how our siamese network performs on N-way One-shot classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate our Siamese Network on N-way One-shot classification\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=2,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g_seed\n",
    "    )\n",
    "\n",
    "evaluate_one_shot(siamese_net, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Congratulations! We successfully solved our first Meta-Learning problem. Feel free to run through this section again with different parameters and compare the differences!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Bonus Section 1.5: Section Summary\n",
    "\n",
    "In this section, we learnt the following (try to answer these questions to ensure that you understood the content fairly well):\n",
    "\n",
    "* What is Meta-Learning and how is it used to solve various problems with Deep Learning?\n",
    "\n",
    "* What is Few-shot Learning? We took a look at what is N-way K-shot classification using the Omniglot dataset and Convolutional Siamese Networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Bonus 2: Continual and Life-Long Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 5: Continual and Life-Long Learning\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1uX4y1c7Yh\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"-lmY_lc75As\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# add event to airtable\n",
    "atform.add_event('Video 5: Continual and Life-Long Learning')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Bonus Think! 2: Why are Continual and Life-Long Learning tough?\n",
    "\n",
    "Take a moment to think about the following:\n",
    "\n",
    "1. You have had an extensive tutorial on Continual Learning. Can you quickly recall the different problems with Continual Learning and how these problems were solved?\n",
    "2. What do you think makes Life-Long Learning tough to implement? How is this learning paradigm different from those that you have seen before? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Bonus 3 - Feature Importance Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "If you wish to understand how the accuracies are calculated in Section 3.3, continue reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Recall the problem setup:\n",
    "\n",
    "There exist binary-valued random variables $X^{(1)}, X^{(2)}, X^{(3)}, Y \\in {0,1}$ such that $X^{(1)}, X^{(2)}$, and $X^{(3)}$ are conditionally independent (given $Y$).\n",
    "\n",
    "Let $P(Y=1) = 1/2$. Then the joint distribution of $X^{(1)}, X^{(2)}, X^{(3)}, Y$ is specified by the conditional probabilities $P(X_i=1 | Y=0)$ and $P(X_i=1 | Y=1)$, for $i=1,2,3$:\n",
    "\n",
    "\\begin{align}\n",
    "P(X^{(1)}=1|Y=0)=0.10 &, P(X^{(1)}=1|Y=1)=0.0.90 \\\\\n",
    "P(X^{(2)}=1|Y=0)=0.05 &, P(X^{(2)}=1|Y=1)=0.0.80 \\\\\n",
    "P(X^{(3)}=1|Y=0)=0.01 &, P(X^{(1)}=1|Y=1)=0.0.71\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Define a feature importance metric\n",
    "\n",
    "Here we'll use 0-1 loss to define feature importance. We can imagine using $X^{(1)}, X^{(2)}$, and $X^{(3)}$ to predict $Y$. $Y$ can only be 0 or 1, so your predictions must be 0 or 1 and will either be right or wrong (i.e, perfect for 0-1 loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Calculate the individual feature importance (0-1 loss) for  $X^{(1)}$, $X^{(2)}$, and $X^{(3)}$\n",
    "\n",
    "**Hint:** You can accomplish this by finding the probability that  X  and  Y  match. You're given $P(X^{(1)}=1|Y=0)$, but you need $P(X^{(1)}=0|Y=0)$ . Recall that $P(Y)=1/2$ and compute an equally weighted average of the two accuracies you can compute given the two possible values of $X^{(1)}$ .\n",
    "\n",
    "**Solution:** If $P(X^{(1)} = 1|Y = 0)=0.10$, then $P(X^{(1)} = 0|Y = 0)=0.90$. \n",
    "\n",
    "There's a symmetry we can take advantage of here: we know that 90% of the time $Y=1$ we have $X=1$, which implies that 90% of the time $X=1$ we have $Y=1$, which is a more natural way of thinking about prediction.\n",
    "\n",
    "This means we predict correctly 90% of the time when $Y=0$, and we predict correctly 90% of the time when $Y=1$. Since $P(Y)=1/2$, we have 0-1 loss $L = 1-(0.5 \\times 0.9 + 0.5 \\times 0.9)=0.1$.\n",
    "\n",
    "In total, we have:\n",
    "\n",
    "\\begin{align}\n",
    "L_1 =& 1 - (0.5 \\times 0.9 + 0.5 \\times 0.9) = 0.1 \\\\\n",
    "L_2 =& 1 - (0.5 \\times 0.95 + 0.5 \\times 0.8) = 0.125 \\\\\n",
    "L_3 =& 1 - (0.5 \\times 0.99 + 0.5 \\times 0.71) = 0.15.\n",
    "\\end{align}\n",
    "\n",
    "**Check:** If you did not observe that feature $X^{(1)}$ is the most important feature by virtue of having the smallest 0-1 loss (0.1), check your calculations before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Calculate the importance of feature pairs (0-1 loss) for $\\{X^{(1)}, X^{(2)}\\}$, $\\{X^{(1)}, X^{(3)}\\}$, and $\\{X^{(2)}, X^{(3)}\\}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Let's start with $\\{X^{(1)}, X^{(2)}\\}$\n",
    "\n",
    "Working out these calculations is a bit tedious, so we'll provide various tables and descriptions of their origins.\n",
    "\n",
    "First, we compute conditional PMFs $P\\left( X^{(1)}, X^{(2)}∣Y \\right)$ by multiplying the provided conditionally independent probabilities together.\n",
    "\n",
    "**Conditional PMFs:**\n",
    "\n",
    "| $P(X_1,X_2|Y=0)$ |$X_2=0$  |$X_2=1$ |\n",
    "|------------------|---------|--------|\n",
    "| $X_1=0$          | 0.855   | 0.045  |\n",
    "| $X_1=1$          | 0.095   | 0.005  |\n",
    "\n",
    "| $P(X_1,X_2|Y=1)$ |$X_2=0$  |$X_2=1$ |       \n",
    "|------------------|---------|--------|\n",
    "| $X_1=0$          | 0.02    | 0.08   |\n",
    "| $X_1=1$          | 0.18    | 0.72   |\n",
    "\n",
    "\n",
    "We can then obtain a marginal PMF for $\\{X^{(1)}, X^{(2)}\\}$ by element-wise averaging these two 2x2 tables because $P(Y)=1/2$.\n",
    "\n",
    "**Marginal PMF:**\n",
    "\n",
    "| $P(X_1,X_2)$ |$X_2=0$  |$X_2=1$ |       \n",
    "|--------------|---------|--------|\n",
    "| $X_1=0$      | 0.4375  | 0.0625 |       \n",
    "| $X_1=1$      | 0.1375  | 0.3625 |\n",
    "\n",
    "\n",
    "We now use the conditional PMFs to evaluate the conditional probability expressions we care about: $P\\left( Y∣X^{(1)}, X^{(2)} \\right)$.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Conditional Probabilities: $P \\left( Y=\\{0, 1\\}|X_1,X_2 \\right)$**\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{P(X_1,X_2 | Y=0)}{P(X_1,X_2|Y=0) + P(X_1,X_2|Y=1)}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "| $P(Y=0|X_1,X_2)$ |$X_2=0$   |$X_2=1$   |\n",
    "|------------------|----------|----------|\n",
    "| $X_1=0$          | 0.977143 | 0.36     |\n",
    "| $X_1=1$          | 0.345455 | 0.006897 |\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{P(X_1, X_2 | Y=1)}{P(X_1,X_2|Y=0) + P(X_1,X_2|Y=1)}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "| $P(Y=1|X_1,X_2)$ |$X_2=0$    |$X_2=1$   |\n",
    "|------------------|-----------|----------|\n",
    "| $X_1=0$          | 0.022857  | 0.64     |\n",
    "| $X_1=1$          | 0.654545  | 0.993103 |\n",
    "\n",
    "<br>\n",
    "\n",
    "We now implement a 0-1 classifier that predicts $\\hat{Y}=0$ or $\\hat{Y}=1$ based on the element-wise maximum in these two tables. For example, when $X^{(1)}=0$ and $X^{(2)}=0$, there's a 0.977 probability that $Y=0$ and a 0.023 probability that $Y=1$. We would of course predict $\\hat{Y}=1$ , and we expect to be correct 97.7% of the time.\n",
    "\n",
    "We now know the expected probability of success for our classifier for each $\\{X^{(1)}, X^{(2)}\\}$ combination.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Maximum Conditional Probability:**\n",
    "\n",
    "| $P(\\text{Correct}|X_1,X_2)$ |$X_2=0$   |$X_2=1$   |\n",
    "|-----------------------------|----------|----------|\n",
    "| $X_1=0$                     | 0.977143 | 0.64     |\n",
    "| $X_1=1$                     | 0.654545 | 0.993103 |\n",
    "\n",
    "\n",
    "Only one task remains for you to complete: compute the expected 0-1 loss for this classifier.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Hint:** We know how likely we are to be correct given some $\\{X(1), X(2)\\}$ combination. We also know how likely those combinations are to appear.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "marginal = np.array([[0.4375, 0.0625],\n",
    "                     [0.1375, 0.3625]])\n",
    "max_cond_prob = np.array([[0.977, 0.64],\n",
    "                          [0.6545, 0.993]])\n",
    "# We need the sum-product of these two arrays.\n",
    "# (1) element-wise multiplication to produce one array.\n",
    "# (2) sum the elements of the resulting array\n",
    "accuracy = np.sum(np.multiply(marginal, max_cond_prob))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# 0-1 loss is 1 - accuracy.\n",
    "print(1 - accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Check:** You should have gotten 0-1 loss for $\\{X^{(1)}, X^{(2)}\\}$ to be 0.0826."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Let's move on to  $\\{X^{(2)}, X^{(3)}\\}$\n",
    "\n",
    "Our claim was that the most important single feature, already found to be $\\{X^{(1)}$\\}, is not in the most important pair of features, so we just need to show that the 0-1 loss for $\\{X^{(2)}, X^{(3)}\\}$ is less than 0.0826.\n",
    "\n",
    "First, we compute conditional PMFs $P \\left( X^{(2)}, X^{(3)}∣Y \\right)$ by multiplying conditionally independent probabilities together.\n",
    "\n",
    "**Conditional PMFs:**\n",
    "\n",
    "| $P(X_2,X_3|Y=0)$ |$X_3=0$  |$X_3=1$ |\n",
    "|------------------|---------|--------|\n",
    "| $X_2=0$          | 0.9405  | 0.0095 |\n",
    "| $X_2=1$          | 0.0495  | 0.0005 |\n",
    "\n",
    "| $P(X_2,X_3|Y=1)$ |$X_3=0$  |$X_3=1$ | \n",
    "|-----------------|---------|--------|\n",
    "| $X_2=0$          | 0.058   | 0.142  |\n",
    "| $X_2=1$          | 0.232   | 0.568  |\n",
    "\n",
    "\n",
    "We can then obtain a marginal PMF for $\\{X^{(2)}, X^{(3)|\\}$ by element-wise averaging these two 2x2 tables because $P(Y)=1/2$.\n",
    "\n",
    "**Marginal PMF:**\n",
    "\n",
    "| $P(X_2,X_3)$   |$X_3=0$    |$X_3=1$  |\n",
    "|----------------|-----------|---------|\n",
    "| $X_2=0$        | 0.49925   | 0.07575 |\n",
    "| $X_2=1$        | 0.14075   | 0.28425 |\n",
    "\n",
    "\n",
    "We now use the conditional PMFs to evaluate the conditional probability expressions we care about: $P \\left( Y∣X^{(2)}, X^{(3)} \\right)$.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Conditional Probabilities: $P \\left( Y=\\{0, 1\\}|X_2, X_3 \\right)$**\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{P(X_2,X_3 | Y=0)}{P(X_2, X_3|Y=0) + P(X_2, X_3|Y=1)}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "| $P(Y=0|X_2,X_3)$ | $X_3=0$     |$X_3=1$      |   \n",
    "|------------------|-------------|-------------|\n",
    "| $X_2=0$          | 0.941912869 | 0.062706271 |\n",
    "| $X_2=1$          | 0.175843694 | 0.000879507 |\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{P(X_2,X_3 | Y=1)}{P(X_2, X_3|Y=0) + P(X_2, X_3|Y=1)}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "| $P(Y=0|X_2,X_3)$ | $X_3=0$     |$X_3=1$      |   \n",
    "|------------------|-------------|-------------|\n",
    "| $X_2=0$          | 0.058087131 | 0.937293729 |\n",
    "| $X_2=1$          | 0.824156306 | 0.999120493 |\n",
    "\n",
    "<br>\n",
    "\n",
    "We now implement a 0-1 classifier that predicts $\\hat{Y}=0$ or $\\hat{Y}=1$ based on the element-wise maximum in these two tables. For example, when $X^{(2)}=0$ and $X^{(3)}=0$, there's a 0.942 probability that $Y=0$ and a 0.058 probability that $Y=1$. We would of course predict $\\hat{Y}=1$ , and we expect to be correct 94.2% of the time.\n",
    "\n",
    "We now know the expected probability of success for our classifier for each $\\{X^{(2)}, X^{(3)}\\}$ combination.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Maximum Conditional Probability:**\n",
    "\n",
    "| $P(\\text{Correct}|X_2,X_3)$ |$X_3=0$      |$X_3=1$      |\n",
    "|-----------------------------|-------------|-------------|\n",
    "| $X_1=0$                     | 0.941912869 | 0.937293729 |\n",
    "| $X_1=1$                     | 0.824156306 | 0.999120493 |\n",
    "\n",
    "\n",
    "Only one task remains for you to complete: compute the expected 0-1 loss for this classifier.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Hint:** We know how likely we are to be correct given some $\\{X(2), X(3)\\}$ combination. We also know how likely those combinations are to appear.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "marginal = np.array([[0.49925, 0.07575],\n",
    "                     [0.14075, 0.28425]])\n",
    "max_cond_prob = np.array([[0.941912869, 0.937293729],\n",
    "                          [0.824156306,0.999120493]])\n",
    "# We need the sum-product of these two arrays.\n",
    "# (1) element-wise multiplication to produce one array.\n",
    "# (2) sum the elements of the resulting array\n",
    "accuracy = np.sum(np.multiply(marginal, max_cond_prob))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# 0-1 loss is 1 - accuracy.\n",
    "print(1 - accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Check:** You should have gotten 0-1 loss for $\\{X^{(2)}, X^{(3)}\\}$ to be 0.05875."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "We demonstrated that our intuition could be misleading - it's not always the case that a feature that ranks highly among individual features would necessarily warrant being included at all when larger sets of features are considered.\n",
    "\n",
    "Something to think about: What does this tell you about stepwise feature selection approaches?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W3D4_Tutorial2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
