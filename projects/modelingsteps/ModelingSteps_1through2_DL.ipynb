{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/projects/modelingsteps/ModelingSteps_1through2_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/projects/modelingsteps/ModelingSteps_1through2_DL.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Modeling Steps 1 - 2\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Marius 't Hart, Megan Peters, Paul Schrater, Gunnar Blohm\n",
    "\n",
    "__Content reviewers:__ Eric DeWitt, Tara van Viegen, Marius Pachitariu\n",
    "\n",
    "__Production editors:__ Ella Batty, Spiros Chavlis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
    "\n",
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Note that this is the same as [NMA-CN W1D2 Tutorial 1](https://github.com/NeuromatchAcademy/course-content/blob/master/tutorials/W1D2_ModelingPractice/W1D2_Tutorial1.ipynb) - we provide it here as well for ease of access.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Objectives\n",
    "We deconstruct the modeling process and break it down into 10 easy steps. Following the thought process of these steps will help you design and complete a Deep Learning (DL) project.\n",
    "\n",
    "We assume that you have a general idea of a project in mind, i.e., a preliminary question, goal, and/or phenomenon you would like to investigate. These 10 steps were originally developed for computational neuroscience models; but they really apply to any research project. We will now work through the 10 steps of modeling ([Blohm et al., 2019](https://doi.org/10.1523/ENEURO.0352-19.2019)).\n",
    "\n",
    "We provide 3 example projects:\n",
    "\n",
    "* a neuro theory model (if you're comp neuro inclined) - this is also our roleplay example! See the corresponding notebook [here](https://github.com/NeuromatchAcademy/course-content-dl/blob/main/projects/modelingsteps/TrainIllusionModelingProjectDL.ipynb)\n",
    "* a brain decoding model (simple logistic regression; if your data science inclined)! See the corresponding notebook [here](https://github.com/NeuromatchAcademy/course-content-dl/blob/main/projects/modelingsteps/TrainIllusionDataProjectDL.ipynb)\n",
    "* a movement classification model (Convolutional Neural Network; if you're DL inclined)! See the corresponding notebook [here](https://github.com/NeuromatchAcademy/course-content-dl/blob/main/projects/modelingsteps/Example_Deep_Learning_Project.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c9b77b8dd648fda78acd423fb4adce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output()), _titles={'0': 'Youtube', '1': 'Bilibili'})"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Video 0: 10 steps overview\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = 'https://player.bilibili.com/player.html?bvid={0}&page={1}'.format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=\"BV1uw411R7RR\", width=854, height=480, fs=1)\n",
    "  print('Video available at https://www.bilibili.com/video/{0}'.format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=\"9gw2lmnHY54\", width=854, height=480, fs=1, rel=0)\n",
    "  print('Video available at https://youtube.com/watch?v=' + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"854\"\n",
       "            height=\"480\"\n",
       "            src=\"https://mfr.ca-1.osf.io/render?url=https://osf.io/wm2q3/?direct%26mode=render%26action=download%26mode=render\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fec05decc10>"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Tutorial slides\n",
    "\n",
    "# @markdown These are the slides for the *DL projects intro*\n",
    "from IPython.display import IFrame\n",
    "IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/wm2q3/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for random distributions:\n",
    "from scipy.stats import norm, poisson\n",
    "\n",
    "# for logistic regression:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plotting Functions\n",
    "\n",
    "def rasterplot(spikes,movement,trial):\n",
    "\n",
    "  [movements, trials, neurons, timepoints] = np.shape(spikes)\n",
    "\n",
    "  trial_spikes = spikes[movement,trial,:,:]\n",
    "\n",
    "  trial_events = [((trial_spikes[x,:] > 0).nonzero()[0]-150)/100 for x in range(neurons)]\n",
    "\n",
    "  plt.figure()\n",
    "  dt=1/100\n",
    "  plt.eventplot(trial_events, linewidths=1);\n",
    "  plt.title('movement: %d - trial: %d'%(movement, trial))\n",
    "  plt.ylabel('neuron')\n",
    "  plt.xlabel('time [s]')\n",
    "\n",
    "def plotCrossValAccuracies(accuracies):\n",
    "  f, ax = plt.subplots(figsize=(8, 3))\n",
    "  ax.boxplot(accuracies, vert=False, widths=.7)\n",
    "  ax.scatter(accuracies, np.ones(8))\n",
    "  ax.set(\n",
    "    xlabel=\"Accuracy\",\n",
    "    yticks=[],\n",
    "    title=f\"Average test accuracy: {accuracies.mean():.2%}\"\n",
    "  )\n",
    "  ax.spines[\"left\"].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Generate Data\n",
    "\n",
    "# @markdown `generateSpikeTrains(seed=37)`\n",
    "\n",
    "# @markdown `subsetPerception(spikes, seed=0)`\n",
    "\n",
    "def generateSpikeTrains(seed=37):\n",
    "\n",
    "  gain = 2\n",
    "  neurons = 50\n",
    "  movements = [0, 1, 2]\n",
    "  repetitions = 800\n",
    "\n",
    "  np.random.seed(seed)\n",
    "\n",
    "  # set up the basic parameters:\n",
    "  dt = 1/100\n",
    "  start, stop = -1.5, 1.5\n",
    "  t = np.arange(start, stop + dt, dt)  # a time interval\n",
    "  Velocity_sigma = 0.5  # std dev of the velocity profile\n",
    "  Velocity_Profile = norm.pdf(t, 0, Velocity_sigma)/norm.pdf(0, 0, Velocity_sigma)  # The Gaussian velocity profile, normalized to a peak of 1\n",
    "\n",
    "  # set up the neuron properties:\n",
    "  Gains = np.random.rand(neurons) * gain  # random sensitivity between 0 and `gain`\n",
    "  FRs   = (np.random.rand(neurons) * 60 ) - 10  # random base firing rate between -10 and 50\n",
    "\n",
    "  # output matrix will have this shape:\n",
    "  target_shape = [len(movements), repetitions, neurons, len(Velocity_Profile)]\n",
    "\n",
    "  # build matrix for spikes, first, they depend on the velocity profile:\n",
    "  Spikes = np.repeat(Velocity_Profile.reshape([1, 1, 1, len(Velocity_Profile)]),\n",
    "                     len(movements)*repetitions*neurons, axis=2).reshape(target_shape)\n",
    "\n",
    "  # multiplied by gains:\n",
    "  S_gains = np.repeat(np.repeat(Gains.reshape([1, 1, neurons]),\n",
    "                                len(movements)*repetitions, axis=1).reshape(target_shape[:3]),\n",
    "                      len(Velocity_Profile)).reshape(target_shape)\n",
    "  Spikes = Spikes * S_gains\n",
    "\n",
    "  # and multiplied by the movement:\n",
    "  S_moves = np.repeat( np.array(movements).reshape([len(movements), 1, 1, 1]),\n",
    "                      repetitions*neurons*len(Velocity_Profile),\n",
    "                      axis=3 ).reshape(target_shape)\n",
    "\n",
    "  Spikes = Spikes * S_moves\n",
    "\n",
    "  # on top of a baseline firing rate:\n",
    "  S_FR = np.repeat(np.repeat(FRs.reshape([1, 1, neurons]),\n",
    "                             len(movements)*repetitions, axis=1).reshape(target_shape[:3]),\n",
    "                   len(Velocity_Profile)).reshape(target_shape)\n",
    "  Spikes = Spikes + S_FR\n",
    "\n",
    "  # can not run the poisson random number generator on input lower than 0:\n",
    "  Spikes = np.where(Spikes < 0, 0, Spikes)\n",
    "\n",
    "  # so far, these were expected firing rates per second, correct for dt:\n",
    "  Spikes = poisson.rvs(Spikes * dt)\n",
    "\n",
    "  return Spikes\n",
    "\n",
    "\n",
    "def subsetPerception(spikes, seed=0):\n",
    "\n",
    "  movements = [0, 1, 2]\n",
    "  split = 400\n",
    "  subset = 40\n",
    "  hwin = 3\n",
    "\n",
    "  [num_movements, repetitions, neurons, timepoints] = np.shape(spikes)\n",
    "\n",
    "  decision = np.zeros([num_movements, repetitions])\n",
    "\n",
    "  # ground truth for logistic regression:\n",
    "  y_train = np.repeat([0, 1, 1], split)\n",
    "  y_test  = np.repeat([0, 1, 1], repetitions - split)\n",
    "\n",
    "  m_train = np.repeat(movements, split)\n",
    "  m_test  = np.repeat(movements, split)\n",
    "\n",
    "  # reproduce the time points:\n",
    "  dt = 1 / 100\n",
    "  start, stop = -1.5, 1.5\n",
    "  t = np.arange(start, stop+dt, dt)\n",
    "\n",
    "  w_idx = list((abs(t) < (hwin*dt)).nonzero()[0])\n",
    "  w_0 = min(w_idx)\n",
    "  w_1 = max(w_idx) + 1  # python...0\n",
    "\n",
    "  # get the total spike counts from stationary and movement trials:\n",
    "  spikes_stat = np.sum(spikes[0, :, :, :], axis=2)\n",
    "  spikes_move = np.sum(spikes[1:, :, :, :], axis=3)\n",
    "\n",
    "  train_spikes_stat = spikes_stat[:split, :]\n",
    "  train_spikes_move = spikes_move[:, :split, :].reshape([-1 ,neurons])\n",
    "\n",
    "  test_spikes_stat = spikes_stat[split:, :]\n",
    "  test_spikes_move = spikes_move[:, split:, :].reshape([-1, neurons])\n",
    "\n",
    "  # data to use to predict y:\n",
    "  x_train = np.concatenate((train_spikes_stat, train_spikes_move))\n",
    "  x_test = np.concatenate(( test_spikes_stat, test_spikes_move))\n",
    "\n",
    "  # this line creates a logistics regression model object, and immediately fits it:\n",
    "  population_model = LogisticRegression(solver='liblinear',\n",
    "                                        random_state=seed).fit(x_train, y_train)\n",
    "\n",
    "  # solver, one of: 'liblinear', 'newton-cg', 'lbfgs', 'sag', and 'saga'\n",
    "  # some of those require certain other options\n",
    "  # print(population_model.coef_)       # slope\n",
    "  # print(population_model.intercept_)  # intercept\n",
    "\n",
    "  ground_truth = np.array(population_model.predict(x_test))\n",
    "  ground_truth = ground_truth.reshape([3, -1])\n",
    "\n",
    "  output = {}\n",
    "  output['perception'] = ground_truth\n",
    "  output['spikes'] = spikes[:, split:, :subset, :]\n",
    "\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def getData():\n",
    "\n",
    "  spikes = generateSpikeTrains()\n",
    "  dataset = subsetPerception(spikes=spikes)\n",
    "\n",
    "  return dataset\n",
    "\n",
    "\n",
    "dataset = getData()\n",
    "perception = dataset['perception']\n",
    "spikes = dataset['spikes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Demos\n",
    "We will demo the modeling process to you based on the train illusion. The introductory video will explain the phenomenon to you. Then we will do roleplay to showcase some common pitfalls to you based on a computational modeling project around the train illusion. In addition to the computational model, we will also provide a data neuroscience project example to you so you can appreciate similarities and differences. \n",
    "\n",
    "Enjoy!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Disclaimer\n",
    "The pitfalls roleplay videos were developed for a computational neuroscience modeling project. But all steps and pitfalls also apply to Deep Learning projects. There is a DL joke throughout these videos; that does NOT mean we do not like or appreciate DL (on the contrary, otherwise we would not be teaching it here). But it should serve as a warning that DL is not a magic answer to all questions... 😉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9893de687447f2ae7587b2f1e318c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output()), _titles={'0': 'Youtube', '1': 'Bilibili'})"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Video 1: Introduction to tutorial\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = 'https://player.bilibili.com/player.html?bvid={0}&page={1}'.format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=\"BV1Mf4y1b7xS\", width=854, height=480, fs=1)\n",
    "  print('Video available at https://www.bilibili.com/video/{0}'.format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=\"GyGNs1fLIYQ\", width=854, height=480, fs=1, rel=0)\n",
    "  print('Video available at https://youtube.com/watch?v=' + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "----\n",
    "# Step 1: Finding a phenomenon and a question to ask about it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88b75119a2b43b99d68aec904175117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output()), _titles={'0': 'Youtube', '1': 'Bilibili'})"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Video 2: Asking a question\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = 'https://player.bilibili.com/player.html?bvid={0}&page={1}'.format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=\"BV1VK4y1M7dc\", width=854, height=480, fs=1)\n",
    "  print('Video available at https://www.bilibili.com/video/{0}'.format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=\"4Gl8X_y_uoA\", width=854, height=480, fs=1, rel=0)\n",
    "  print('Video available at https://youtube.com/watch?v=' + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc33e59dd9954cfabe37c277ab31fcd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output(), Output()), _titles={'0': 'Computational Model', '1': 'Data Analysis', '2': '…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Example projects step 1\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import Markdown\n",
    "\n",
    "markdown1 = '''\n",
    "\n",
    "## Step 1\n",
    "<br>\n",
    "<font size='3pt'>\n",
    "The train illusion occurs when sitting on a train and viewing another train outside the window. Suddenly, the other train *seems* to move, i.e. you experience visual motion of the other train relative to your train. But which train is actually moving?\n",
    "\n",
    "Often people have the wrong percept. In particular, they think their own train might be moving when it's the other train that moves; or vice versa. The illusion is usually resolved once you gain vision of the surroundings that lets you disambiguate the relative motion; or if you experience strong vibrations indicating that it is indeed your own train that is in motion.\n",
    "\n",
    "\n",
    "We asked the following (arbitrary) question for our demo project: \"How do noisy vestibular estimates of motion lead to illusory percepts of self motion?\"\n",
    "</font>\n",
    "'''\n",
    "\n",
    "markdown2 = '''\n",
    "## Step 1\n",
    "<br>\n",
    "\n",
    "<font size='3pt'>\n",
    "The train illusion occurs when sitting on a train and viewing another train outside the window. Suddenly, the other train *seems* to move, i.e. you experience visual motion of the other train relative to your train. But which train is actually moving?\n",
    "\n",
    "Often people mix this up. In particular, they think their own train might be moving when it's the other train that moves; or vice versa. The illusion is usually resolved once you gain vision of the surroundings that lets you disambiguate the relative motion; or if you experience strong vibrations indicating that it is indeed your own train that is in motion.\n",
    "\n",
    "\n",
    "We assume that we have build the train illusion model (see the other example project colab). That model predicts that accumulated sensory evidence from vestibular signals determines the decision of whether self-motion is experienced or not. We now have vestibular neuron data (simulated in our case, but let's pretend) and would like to see if that prediction holds true.\n",
    "\n",
    "The data contains *N* neurons and *M* trials for each of 3 motion conditions: no self-motion, slowly accelerating self-motion and faster accelerating self-motion. In our data,\n",
    "*N* = 40 and *M* = 400.\n",
    "\n",
    "**So we can ask the following question**: \"Does accumulated vestibular neuron activity correlate with self-motion judgements?\"\n",
    "</font>\n",
    "'''\n",
    "\n",
    "markdown3 = '''\n",
    "## Step 1\n",
    "<br>\n",
    "<font size='3pt'>\n",
    "There are many different questions we could ask with the MoVi dataset. We will start with a simple question: \"Can we classify movements from skeletal motion data, and if so, which body parts are the most informative ones?\"\n",
    "\n",
    "Our goal is to perform a pilot study to see if this is possible in principle. We will therefore use \"ground truth\" skeletal motion data that has been computed using an inference algorithm (see MoVi paper). If this works out, then as a next step we might want to use the raw sensor data or even videos...\n",
    "\n",
    "The ultimate goal could for example be to figure out which body parts to record movements from (e.g. is just a wristband enough?) to classify movement.\n",
    "</font>\n",
    "'''\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  display(Markdown(markdown2))\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  display(Markdown(markdown1))\n",
    "\n",
    "out3 = widgets.Output()\n",
    "with out3:\n",
    "  display(Markdown(markdown3))\n",
    "\n",
    "out = widgets.Tab([out1, out2, out3])\n",
    "out.set_title(0, 'Computational Model')\n",
    "out.set_title(1, 'Data Analysis')\n",
    "out.set_title(2, 'Deep Learning')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Asking your own question \n",
    "\n",
    "You should already have a project idea from your brainstorming yesterday. **Write down the phenomenon, question and goal(s) if you have them.** \n",
    "\n",
    "As a reminder, here is what you should discuss and write down:\n",
    "\n",
    "* What exact aspect of data needs modeling?\n",
    "  * Answer this question clearly and precisely!\n",
    "Otherwise you will get lost (almost guaranteed)\n",
    "  * Write everything down!\n",
    "  * Also identify aspects of data that you do not want to address (yet)\n",
    "* Define an evaluation method!\n",
    "  * How will you know your modeling is good?\n",
    "  * E.g., comparison to specific data (quantitative method of comparison?)\n",
    "* For computational models: think of an experiment that could test your model\n",
    "  * You essentially want your model to interface with this experiment, i.e. you want to simulate this experiment\n",
    "\n",
    "You can find interesting questions by looking for phenomena that differ from your expectations. In *what* way does it differ? *How* could that be explained (starting to think about mechanistic questions and structural hypotheses)? *Why* could it be the way it is? What experiment could you design to investigate this phenomenon? What kind of data would you need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Make sure to avoid the pitfalls!**\n",
    "<details>\n",
    "<summary>Click here for a recap on pitfalls</summary>\n",
    "\n",
    "Question is too general\n",
    "<ul>\n",
    "  <li>Remember: science advances one small step at the time. Get the small step right…</li>\n",
    "  </ul>\n",
    "\n",
    "Precise aspect of phenomenon you want to model is unclear\n",
    "  <ul>\n",
    "  <li>You will fail to ask a meaningful question</li>\n",
    "  </ul>\n",
    "\n",
    "You have already chosen a toolkit\n",
    "  <ul>\n",
    "  <li>This will prevent you from thinking deeply about the best way to answer your scientific question</li>\n",
    "  </ul>\n",
    "\n",
    "You don’t have a clear goal\n",
    "  <ul>\n",
    "  <li>What do you want to get out of modeling?</li>\n",
    "  </ul>\n",
    "\n",
    "You don’t have a potential experiment in mind\n",
    "  <ul>\n",
    "  <li>This will help concretize your objectives and think through the logic behind your goal</li>\n",
    "  </ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Note**\n",
    "\n",
    "The hardest part is Step 1. Once that is properly set up, all other should be easier. **BUT**: often you think that Step 1 is done only to figure out in later steps (anywhere really) that you were not as clear on your question and goal than you thought. Revisiting Step 1 is frequent necessity. Don't feel bad about it. You can revisit Step 1 later; for now, let's move on to the nest step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "----\n",
    "# Step 2: Understanding the state of the art & background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Here you will do a literature review (**to be done AFTER this tutorial!**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9983118ea53d48a59dd825e061d292ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output()), _titles={'0': 'Youtube', '1': 'Bilibili'})"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Video 3: Literature Review & Background Knowledge\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = 'https://player.bilibili.com/player.html?bvid={0}&page={1}'.format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=\"BV1by4y1M7TZ\", width=854, height=480, fs=1)\n",
    "  print('Video available at https://www.bilibili.com/video/{0}'.format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=\"d8zriLaMc14\", width=854, height=480, fs=1, rel=0)\n",
    "  print('Video available at https://youtube.com/watch?v=' + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab25162fb01417d96c6530b52fd974d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output(), Output()), _titles={'0': 'Computational Model', '1': 'Data Analysis', '2': '…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Example projects step 2\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import Markdown\n",
    "import numpy as np\n",
    "\n",
    "markdown1 = '''\n",
    "\n",
    "## Step 2\n",
    "\n",
    "<br>\n",
    "<font size='3pt'>\n",
    "\n",
    "You have learned all about the vestibular system in the Intro video. This is also where you would do a literature search to learn more about what's known about self-motion perception and vestibular signals. You would also want to examine any attempts to model self-motion, perceptual decision making and vestibular processing.</font>\n",
    "'''\n",
    "\n",
    "markdown21 = '''\n",
    "## Step 2\n",
    "\n",
    "<br>\n",
    "<font size='3pt'>\n",
    "While it seems a well-known fact that vestibular signals are noisy, we should check if we can also find this in the literature.\n",
    "\n",
    "Let's also see what's in our data, there should be a 4d array called `spikes` that has spike counts (positive integers), a 2d array called `perception` with self-motion judgements (0=no motion or 1=motion). Let's see what this data looks like:\n",
    "\n",
    "</font><br>\n",
    "'''\n",
    "\n",
    "markdown22 = '''\n",
    "<br>\n",
    "<font size='3pt'>\n",
    "In the `spikes` array, we see our 3 acceleration conditions (first dimension), with 400 trials each (second dimensions) and simultaneous recordings from 40 neurons (third dimension), across 3 seconds in 10 ms bins (fourth dimension). The first two dimensions are also there in the `perception` array.\n",
    "\n",
    "Perfect perception would have looked like [0, 1, 1]. The average judgements are far from correct (lots of self-motion illusions) but they do make some sense: it's closer to 0 in the no-motion condition and closer to 1 in both of the real-motion conditions.\n",
    "\n",
    "The idea of our project is that the vestibular signals are noisy so that they might be mis-interpreted by the brain. Let's see if we can reproduce the stimuli from the data:\n",
    "</font>\n",
    "<br>\n",
    "'''\n",
    "\n",
    "markdown23 = '''\n",
    "<br>\n",
    "<font size='3pt'>\n",
    "Blue is the no-motion condition, and produces flat average spike counts across the 3 s time interval. The orange and green line do show a bell-shaped curve that corresponds to the acceleration profile. But there also seems to be considerable noise: exactly what we need. Let's see what the spike trains for a single trial look like:\n",
    "</font>\n",
    "<br>\n",
    "'''\n",
    "\n",
    "markdown24 = '''\n",
    "<br>\n",
    "<font size='3pt'>\n",
    "You can change the trial number in the bit of code above to compare what the rasterplots look like in different trials. You'll notice that they all look kind of the same: the 3 conditions are very hard (impossible?) to distinguish by eye-balling.\n",
    "\n",
    "Now that we have seen the data, let's see if we can extract self-motion judgements from the spike counts.\n",
    "</font>\n",
    "<br>\n",
    "'''\n",
    "\n",
    "display(Markdown(r\"\"))\n",
    "\n",
    "\n",
    "markdown3 = '''\n",
    "\n",
    "## Step 2\n",
    "\n",
    "<br>\n",
    "<font size='3pt'>\n",
    "\n",
    "Most importantly, our literature review needs to address the following:\n",
    "* what modeling approaches make it possible to classify time series data?\n",
    "* how is human motion captured?\n",
    "* what exactly is in the MoVi dataset?\n",
    "* what is known regarding classification of human movement based on different measurements?\n",
    "\n",
    "What we learn from the literature review is too long to write out here... But we would like to point out that human motion classification has been done; we're not proposing a very novel project here. But that's ok for an NMA project!\n",
    "</font>\n",
    "<br>\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  display(Markdown(markdown21))\n",
    "  print(f'The shape of `spikes` is: {np.shape(spikes)}')\n",
    "  print(f'The shape of `perception` is: {np.shape(perception)}')\n",
    "  print(f'The mean of `perception` is: {np.mean(perception, axis=1)}')\n",
    "  display(Markdown(markdown22))\n",
    "\n",
    "  for move_no in range(3):\n",
    "    plt.plot(np.arange(-1.5, 1.5 + (1/100),\n",
    "                       (1/100)),\n",
    "             np.mean(np.mean(spikes[move_no, :, :, :],\n",
    "                             axis=0),\n",
    "                     axis=0),\n",
    "             label=['no motion', '$1 m/s^2$', '$2 m/s^2$'][move_no])\n",
    "  plt.xlabel('time [s]');\n",
    "  plt.ylabel('averaged spike counts');\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "  display(Markdown(markdown23))\n",
    "\n",
    "  for move in range(3):\n",
    "    rasterplot(spikes = spikes, movement = move, trial = 0)\n",
    "  plt.show()\n",
    "\n",
    "  display(Markdown(markdown24))\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  display(Markdown(markdown1))\n",
    "\n",
    "out3 = widgets.Output()\n",
    "with out3:\n",
    "  display(Markdown(markdown3))\n",
    "\n",
    "out = widgets.Tab([out1, out2, out3])\n",
    "out.set_title(0, 'Computational Model')\n",
    "out.set_title(1, 'Data Analysis')\n",
    "out.set_title(2, 'Deep Learning')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Here you will do a literature review. For the projects, do not spend too much time on this. A thorough literature review could take weeks or months depending on your prior knowledge of the field...\n",
    "\n",
    "The important thing for your project here is not to exhaustively survey the literature but rather to learn the process of modeling. 1-2 days of digging into the literature should be enough!\n",
    "\n",
    "**Here is what you should get out of it**:\n",
    "* Survey the literature\n",
    "  * What’s known?\n",
    "  * What has already been done?\n",
    "  * Previous models as a starting point?\n",
    "  * What hypotheses have been emitted in the field?\n",
    "  * Are there any alternative / complementary modeling approaches?\n",
    "* What skill sets are required?\n",
    "  * Do I need learn something before I can start?\n",
    "  * Ensure that no important aspect is missed\n",
    "* Potentially provides specific data sets / alternative modeling approaches for comparison\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "ModelingSteps_1through2_DL",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
